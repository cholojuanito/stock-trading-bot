{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publicly available trading data for Representatives and Senators\n",
    "\n",
    "This data is made available because of the 2012 STOCK Act.\n",
    "\n",
    "## Data for each group can be found here:\n",
    "- Reps: https://disclosures-clerk.house.gov/FinancialDisclosure\n",
    "- Senators: https://efdsearch.senate.gov/search/home/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data from the House of Representatives\n",
    "We'll start with data from the House of Representatives since they are the larger of the two groups and most of the people whose trading data we care about are Representatives.\n",
    "\n",
    "The data that the Clerk of the House provides are in the form of zip files. The data in this zip files serve as an index to all of the original pdfs that each member must submit.\n",
    "\n",
    "The zip files from the public disclosure website come in either text or xml format.\n",
    "Their schema is the following:\n",
    "- Prefix - the title of the person, nullable\n",
    "- Last - the Representative's last name\n",
    "- First - the Representative's first name\n",
    "- Suffix - the Representative's suffix, nullable\n",
    "- FilingType - one of C, D, P, W, X (more info below)\n",
    "- StateDst - the state and district the person is representing\n",
    "- Year - the year of the filing\n",
    "- FilingDate - the date of the filing\n",
    "- DocID - the internal id of the document, used for downloading the original pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A breakdown of the FilingTypes\n",
    "\n",
    "C - Candidacy Financial Disclosure Report:\n",
    "    Candidates are required to disclose their net worth and assets.\n",
    "    Example: https://disclosures-clerk.house.gov/public_disc/financial-pdfs/2024/10061382.pdf\n",
    "\n",
    "D - Financial Disclosure Report\n",
    "    Candidates are required to disclose if they have receieved more than $5,000 for their campaign.\n",
    "    Example: https://disclosures-clerk.house.gov/public_disc/financial-pdfs/2024/40003638.pdf\n",
    "\n",
    "P - Periodic Transaction Report\n",
    "    Candidates are required to disclose any transactions within 45 days of that transaction.\n",
    "    Example: https://disclosures-clerk.house.gov/public_disc/ptr-pdfs/2024/20025368.pdf\n",
    "\n",
    "W - Withdrawl of Candidacy\n",
    "    Example: https://disclosures-clerk.house.gov/public_disc/financial-pdfs/2024/7923.pdf\n",
    "\n",
    "X - Financial Disclosure Extension Request\n",
    "    Example: https://disclosures-clerk.house.gov/public_disc/financial-pdfs/2024/30022024.pdf\n",
    "\n",
    "**P FilingTypes are what we are most interested in, they provide the trade type,actual stock tickers, general amounts, and dates**\n",
    "\n",
    "### Where are the original pdfs stored?\n",
    "Each pdf is stored at URL that is a combination of the FilingType, Year, and DocID.\n",
    "\n",
    "Base URL for C, D, W, X: https://disclosures-clerk.house.gov/public_disc/financial-pdfs\n",
    "\n",
    "Base URL for P: https://disclosures-clerk.house.gov/public_disc/ptr-pdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download all available Congress people's trading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def download_disclosure_file(session: aiohttp.ClientSession, url: str, year: int, output_dir: Path):\n",
    "    try:\n",
    "        async with session.get(url) as response:\n",
    "            if response.status == 200:\n",
    "                txt_file = f\"{year}FD.txt\"\n",
    "                content = await response.read()\n",
    "                with ZipFile(BytesIO(content)) as zip_file:\n",
    "                    if txt_file in zip_file.namelist():\n",
    "                        zip_file.extract(txt_file, output_dir)\n",
    "                        print(f\"Successfully downloaded and extracted {txt_file}\")\n",
    "                    else:\n",
    "                        print(f\"No {txt_file} found for {url}\")\n",
    "                return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {url}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "async def bulk_download_disclosure_files(base_url: str, years: range, output_dir: Path):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    files_to_download = []\n",
    "    for year in years:\n",
    "        txt_file = f\"{year}FD.txt\"\n",
    "        if (output_dir / txt_file).exists():\n",
    "            print(f\"File {txt_file} already exists, skipping download\")\n",
    "            continue\n",
    "        else:\n",
    "            files_to_download.append((year, txt_file))\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [download_disclosure_file(session, f\"{base_url}/{year}FD.zip\", year, output_dir) \n",
    "                for year, txt_file in files_to_download]\n",
    "        await asyncio.gather(*tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded and extracted 2010FD.txt\n",
      "Successfully downloaded and extracted 2008FD.txt\n",
      "Successfully downloaded and extracted 2009FD.txt\n",
      "Successfully downloaded and extracted 2024FD.txt\n",
      "Successfully downloaded and extracted 2011FD.txt\n",
      "Successfully downloaded and extracted 2012FD.txt\n",
      "Successfully downloaded and extracted 2013FD.txt\n",
      "Successfully downloaded and extracted 2021FD.txt\n",
      "Successfully downloaded and extracted 2022FD.txt\n",
      "Successfully downloaded and extracted 2023FD.txt\n",
      "Successfully downloaded and extracted 2017FD.txt\n",
      "Successfully downloaded and extracted 2016FD.txt\n",
      "Successfully downloaded and extracted 2015FD.txt\n",
      "Successfully downloaded and extracted 2014FD.txt\n",
      "Successfully downloaded and extracted 2018FD.txt\n",
      "Successfully downloaded and extracted 2019FD.txt\n",
      "Successfully downloaded and extracted 2020FD.txt\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://disclosures-clerk.house.gov/public_disc/financial-pdfs\"\n",
    "years = range(2008, 2025)\n",
    "output_dir = Path(\"../data/disclosures\")\n",
    "await bulk_download_disclosure_files(base_url, years, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download all available PTR PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def download_ptr_pdf(session, url: str, output_path: Path) -> bool:\n",
    "    \"\"\"Download a single PTR PDF file asynchronously.\"\"\"\n",
    "    try:\n",
    "        await asyncio.sleep(random.uniform(0.8, 1.3))  # Random delay between requests\n",
    "        async with session.get(url) as response:\n",
    "            if response.status == 200:\n",
    "                output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                content = await response.read()\n",
    "                with open(output_path, 'wb') as f:\n",
    "                    f.write(content)\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"Failed to download {url}: Status {response.status}\")\n",
    "                return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {url}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def parse_filing_data(year: int) -> pd.DataFrame:\n",
    "    \"\"\"Parse the filing data file for a given year and return PTR records.\"\"\"\n",
    "    file_path = Path(f\"../data/disclosures/{year}FD.txt\")\n",
    "    \n",
    "    try:\n",
    "        # Read the tab-separated file\n",
    "        df = pd.read_csv(file_path, sep='\\t')\n",
    "        \n",
    "        # Filter for PTR (Periodic Transaction Report) filings only\n",
    "        ptr_df = df[df['FilingType'] == 'P'].copy()\n",
    "        \n",
    "        # Convert FilingDate to datetime\n",
    "        ptr_df['FilingDate'] = pd.to_datetime(ptr_df['FilingDate'])\n",
    "        \n",
    "        return ptr_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {file_path}: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def generate_pdf_filename(row) -> str:\n",
    "    \"\"\"Generate standardized filename for a PTR filing.\"\"\"\n",
    "    last_name = re.sub(r'[^a-zA-Z]', '', row['Last']).lower()\n",
    "    first_name = re.sub(r'[^a-zA-Z]', '', row['First']).lower()\n",
    "    state_dist = row['StateDst'].lower()\n",
    "    filing_date = row['FilingDate'].strftime('%Y-%m-%d')\n",
    "    \n",
    "    return f\"{last_name}_{first_name}_{state_dist}_{filing_date}.pdf\"\n",
    "\n",
    "async def process_batch(session, batch: list[tuple[str, str]], output_dir: Path) -> tuple[int, int]:\n",
    "    \"\"\"Process a batch of PTR downloads.\"\"\"\n",
    "    tasks = [download_ptr_pdf(session, url, output_dir / filename) \n",
    "             for url, filename in batch\n",
    "             if not (output_dir / filename).exists()]\n",
    "    \n",
    "    if tasks:\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        return sum(1 for r in results if r), len(tasks)\n",
    "    return 0, 0\n",
    "\n",
    "async def download_ptr_pdfs(start_year: int, end_year: int, batch_size: int = 25):\n",
    "    \"\"\"\n",
    "    Download all PTR PDFs for the specified year range.\n",
    "\n",
    "    Args:\n",
    "        start_year (int): The start year of the range.\n",
    "        end_year (int): The end year of the range (inclusive).\n",
    "        batch_size (int): The number of PTRs to download concurrently.\n",
    "    \"\"\"\n",
    "    base_url = \"https://disclosures-clerk.house.gov/public_disc/ptr-pdfs\"\n",
    "    output_dir = Path(\"../data/ptrs\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for year in range(start_year, end_year + 1):\n",
    "            print(f\"Processing year {year}\")\n",
    "            \n",
    "            ptr_df = parse_filing_data(year)\n",
    "            if ptr_df.empty:\n",
    "                continue\n",
    "            \n",
    "            # Create list of all downloads needed\n",
    "            downloads = [\n",
    "                (f\"{base_url}/{year}/{row['DocID']}.pdf\", generate_pdf_filename(row))\n",
    "                for row in ptr_df.to_dict('records')\n",
    "            ]\n",
    "            \n",
    "            # Process in batches\n",
    "            total_successful = 0\n",
    "            total_attempts = 0\n",
    "            \n",
    "            for i in range(0, len(downloads), batch_size):\n",
    "                batch = downloads[i:i + batch_size]\n",
    "                successful, attempts = await process_batch(session, batch, output_dir)\n",
    "                total_successful += successful\n",
    "                total_attempts += attempts\n",
    "                \n",
    "            if total_attempts > 0:\n",
    "                print(f\"Year {year}: Downloaded {total_successful}/{total_attempts} PTR PDFs\")\n",
    "\n",
    "async def download_ptrs(start_year: int, end_year: int, batch_size: int = 25):\n",
    "    \"\"\"Main function to initiate PTR downloads.\"\"\"\n",
    "    await download_ptr_pdfs(start_year, end_year, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing year 2014\n",
      "Year 2014: Downloaded 706/706 PTR PDFs\n",
      "Processing year 2015\n",
      "Year 2015: Downloaded 726/726 PTR PDFs\n",
      "Processing year 2016\n",
      "Year 2016: Downloaded 760/760 PTR PDFs\n",
      "Processing year 2017\n",
      "Year 2017: Downloaded 800/800 PTR PDFs\n",
      "Processing year 2018\n",
      "Year 2018: Downloaded 820/820 PTR PDFs\n",
      "Processing year 2019\n",
      "Year 2019: Downloaded 677/677 PTR PDFs\n",
      "Processing year 2020\n",
      "Year 2020: Downloaded 727/727 PTR PDFs\n",
      "Processing year 2021\n",
      "Year 2021: Downloaded 674/674 PTR PDFs\n",
      "Processing year 2022\n",
      "Year 2022: Downloaded 623/623 PTR PDFs\n",
      "Processing year 2023\n",
      "Year 2023: Downloaded 460/460 PTR PDFs\n",
      "Processing year 2024\n",
      "Year 2024: Downloaded 408/408 PTR PDFs\n"
     ]
    }
   ],
   "source": [
    "await download_ptrs(2014, 2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the PTR PDFs\n",
    "\n",
    "PDFs are notoriously difficult to parse. We'll need to use OCR to get the text and tables from the PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get historic prices for S&P 500 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "def get_spy_data(start_date: str):\n",
    "    # Get SPY data from Yahoo Finance\n",
    "    spy = yf.Ticker(\"SPY\")\n",
    "\n",
    "    # Download historical data from 2000 to present\n",
    "    spy_data = spy.history(\n",
    "        start=start_date,\n",
    "        interval=\"1d\"\n",
    "    )\n",
    "\n",
    "    # Calculate daily returns\n",
    "    spy_data['Returns'] = spy_data['Close'].pct_change()\n",
    "\n",
    "    # Calculate rolling volatility (20-day standard deviation of returns)\n",
    "    spy_data['Volatility_20d'] = spy_data['Returns'].rolling(window=20).std()\n",
    "\n",
    "    # Calculate rolling volatility (60-day standard deviation of returns)\n",
    "    spy_data['Volatility_60d'] = spy_data['Returns'].rolling(window=60).std()\n",
    "\n",
    "    # Add moving averages\n",
    "    spy_data['MA_50'] = spy_data['Close'].rolling(window=50).mean()\n",
    "    spy_data['MA_200'] = spy_data['Close'].rolling(window=200).mean()\n",
    "\n",
    "    # Calculate trading ranges\n",
    "    spy_data['Daily_Range'] = spy_data['High'] - spy_data['Low']\n",
    "    spy_data['Daily_Range_Pct'] = spy_data['Daily_Range'] / spy_data['Open']\n",
    "\n",
    "    # Reset index to make Date a column\n",
    "    spy_data = spy_data.reset_index()\n",
    "\n",
    "    print(f\"Downloaded {len(spy_data)} days of SPY data\")\n",
    "    print(\"\\nRandom sample:\")\n",
    "    print(spy_data.sample(15))\n",
    "    print(\"\\nColumns available:\")\n",
    "    print(spy_data.columns.tolist())\n",
    "\n",
    "    return spy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_output_path = Path(\"../data/spy.csv\")\n",
    "if not spy_output_path.exists():\n",
    "    spy_data = get_spy_data(\"2000-01-01\")\n",
    "    spy_data.to_csv(spy_output_path, index=False)\n",
    "else:\n",
    "    spy_data = pd.read_csv(spy_output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
